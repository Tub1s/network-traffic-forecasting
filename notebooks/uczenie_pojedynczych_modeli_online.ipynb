{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility imports\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import src.features.preprocessing as prep\n",
    "\n",
    "# Math and matrix manipulation imports\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning imports\n",
    "import river\n",
    "from river import ensemble\n",
    "from river import linear_model\n",
    "from river import preprocessing\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./datasets/traffic/\"\n",
    "\n",
    "list_of_datafiles = prep.get_list_of_datafiles(DATA_PATH, sort = True)\n",
    "df = prep.load_data(DATA_PATH, list_of_datafiles)\n",
    "train_df, test_df = prep.make_train_test_datasets(df, split_point=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(A, F):\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "\n",
    "def mda(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_pred[:-1]) == np.sign(y_pred[1:] - y_pred[:-1]).astype(int)))\n",
    "\n",
    "def calculate_metrics(y_real, y_pred):\n",
    "    metrics = {\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_real, y_pred),\n",
    "        \"sMAPE\": smape(y_real, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_real, y_pred),\n",
    "        \"MDA\": mda(y_real, y_pred)\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_incremental_regressor(n_input_size: int, n_output_size: int, target: str):\n",
    "    X_online, y_online = prep.split_sequence(sequence = df[target], n_input_steps=n_input_size, n_output_steps=n_output_size)\n",
    "    X_online = X_online.reshape((X_online.shape[0], X_online.shape[1]))\n",
    "\n",
    "\n",
    "    online_model = river.compose.Pipeline(\n",
    "    river.preprocessing.StandardScaler(),\n",
    "    river.linear_model.LinearRegression(river.optim.SGD(lr=0.00025))\n",
    "    )\n",
    "\n",
    "    online_metric = river.metrics.SMAPE()\n",
    "\n",
    "    online_predicted = list()\n",
    "    for xi, yi in river.stream.iter_array(X_online, y_online):\n",
    "\n",
    "        yi_pred = online_model.predict_one(xi)\n",
    "        online_metric.update(yi[0], yi_pred)\n",
    "        online_predicted.append(yi_pred)\n",
    "        print(f\"ground-truth {yi[0]} | predicted {yi_pred}\")\n",
    "        online_model.learn_one(xi, yi[0])\n",
    "\n",
    "    data = {\n",
    "        \"y_real\": y_online.reshape(y_online.shape[0]),\n",
    "        \"y_pred\": np.ravel(online_predicted),\n",
    "        \"metric\": online_metric\n",
    "    }\n",
    "    \n",
    "    metrics = calculate_metrics(y_online.reshape(y_online.shape[0]), np.ravel(online_predicted))\n",
    "\n",
    "    with open(f\"./results/online_default_in{n_input_size}_out{n_output_size}_t{target.replace('->', '-')}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(metrics, f)\n",
    "\n",
    "    return data, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_incremental_greedy_regressor(n_input_size: int, n_output_size: int, target: str):\n",
    "    X_online, y_online = prep.split_sequence(sequence = df[target], n_input_steps=n_input_size, n_output_steps=n_output_size)\n",
    "    X_online = X_online.reshape((X_online.shape[0], X_online.shape[1]))\n",
    "\n",
    "\n",
    "    online_models = [\n",
    "            river.linear_model.LinearRegression(optimizer=river.optim.SGD(lr=lr))\n",
    "            for lr in [0.0001, 0.00025, 0.001, 1e-05, 0.01]\n",
    "        ]\n",
    "\n",
    "    online_model = (\n",
    "    river.preprocessing.StandardScaler() |\n",
    "        river.model_selection.EpsilonGreedyRegressor(\n",
    "        online_models,\n",
    "        epsilon=0.1,\n",
    "        decay=0.001,\n",
    "        burn_in=100,\n",
    "        seed=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    online_metric = river.metrics.SMAPE()\n",
    "\n",
    "    online_predicted = list()\n",
    "    for xi, yi in river.stream.iter_array(X_online, y_online):\n",
    "        # Test the current model on the new \"unobserved\" sample\n",
    "        yi_pred = online_model.predict_one(xi)\n",
    "\n",
    "        # Update the running metric with the prediction and ground truth value\n",
    "        online_metric.update(yi[0], yi_pred)\n",
    "        online_predicted.append(yi_pred)\n",
    "        print(f\"g-t {yi[0]} | pred {yi_pred}\")\n",
    "        \n",
    "        # Train the model with the new sample\n",
    "        online_model.learn_one(xi, yi[0])\n",
    "\n",
    "    data = {\n",
    "        \"y_real\": y_online,\n",
    "        \"y_pred\": online_predicted,\n",
    "        \"metric\": online_metric\n",
    "    }\n",
    "    \n",
    "    metrics = calculate_metrics(y_online.reshape(y_online.shape[0]), np.ravel(online_predicted))\n",
    "\n",
    "    with open(f\"./results/online_greedy_in{n_input_size}_out{n_output_size}_t{target.replace('->', '-')}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(metrics, f)\n",
    "\n",
    "    return data, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_incremental_bagging_regressor(n_input_size: int, n_output_size: int, target: str):\n",
    "    X_online, y_online = prep.split_sequence(sequence = df[target], n_input_steps=n_input_size, n_output_steps=n_output_size)\n",
    "    X_online = X_online.reshape((X_online.shape[0], X_online.shape[1]))\n",
    "\n",
    "    online_metric = river.metrics.SMAPE()\n",
    "    online_model = preprocessing.StandardScaler()\n",
    "    online_model |= ensemble.BaggingRegressor(\n",
    "        model=linear_model.LinearRegression(intercept_lr=0.00025),\n",
    "        n_models=5,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    online_predicted = list()\n",
    "    for xi, yi in river.stream.iter_array(X_online, y_online):\n",
    "        # Test the current online_model on the new \"unobserved\" sample\n",
    "\n",
    "        yi_pred = online_model.predict_one(xi)\n",
    "\n",
    "        # Update the running online_metric with the prediction and ground truth value\n",
    "        online_metric.update(yi[0], yi_pred)\n",
    "        online_predicted.append(yi_pred)\n",
    "        #print(f\"g-t {yi[0]} | pred {yi_pred}\")\n",
    "\n",
    "        # Train the online_model with the new sample\n",
    "        online_model.learn_one(xi, yi[0])\n",
    "    \n",
    "    data = {\n",
    "        \"y_real\": y_online,\n",
    "        \"y_pred\": online_predicted,\n",
    "        \"metric\": online_metric\n",
    "    }\n",
    "    \n",
    "    metrics = calculate_metrics(y_online.reshape(y_online.shape[0]), np.ravel(online_predicted))\n",
    "\n",
    "    with open(f\"./results/online_bagging_forest_regressor_in{n_input_size}_out{n_output_size}_t{target.replace('->', '-')}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(metrics, f)\n",
    "\n",
    "    return data, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_n_input_steps = [25]\n",
    "list_n_output_steps = [1]\n",
    "list_targets = ['5->8', \"8->5\", \"5->12\", \"8->12\"]\n",
    "\n",
    "# for n_input_steps in list_n_input_steps:\n",
    "#     for n_output_steps in list_n_output_steps:\n",
    "#         for target in list_targets:\n",
    "#             payload, data_metrics = train_incremental_regressor(n_input_steps, n_output_steps, target)\n",
    "            #payload, data_metrics = train_incremental_greedy_regressor(n_input_steps, n_output_steps, target)\n",
    "            #payload, data_metrics = train_incremental_rfr_regressor(n_input_steps, n_output_steps, target)\n",
    "            #payload, data_metrics = train_incremental_bagging_regressor(n_input_steps, n_output_steps, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32d3e74f3df94fb62d833199ff807b97996d62fb18dca42b5a455cf9687e2e8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
